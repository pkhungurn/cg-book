<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="../css/bootstrap.min.css" rel="stylesheet">
  <link href="../css/theme.css" rel="stylesheet">
  <!-- MathJax -->
  <script type="text/javascript">
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        tags: "ams",
      },
      svg: {
        fontCache: 'global'
      },
    };
  </script>
  <script type="text/javascript" id="MathJax-script" src="../mathjax/tex-mml-chtml.js"></script>
  <link rel="stylesheet"
      href="../highlight/styles/default.min.css">
  <script src="../highlight/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
    
  <title>Chapter 7</title>
</head>

<body>
  <div class="container" style="max-width: 640px;">
    <span style="visibility: hidden;">
      \(
      \def\sc#1{\dosc#1\csod}
      \def\dosc#1#2\csod{{\rm #1{\small #2}}}

      \newcommand{\dee}{\mathrm{d}}
      \newcommand{\Dee}{\mathrm{D}}
      \newcommand{\In}{\mathrm{in}}
      \newcommand{\Out}{\mathrm{out}}
      \newcommand{\pdf}{\mathrm{pdf}}
      \newcommand{\Cov}{\mathrm{Cov}}
      \newcommand{\Var}{\mathrm{Var}}

      \newcommand{\ve}[1]{\mathbf{#1}}
      \newcommand{\mrm}[1]{\mathrm{#1}}
      \newcommand{\ves}[1]{\boldsymbol{#1}}
      \newcommand{\etal}{{et~al.}}
      \newcommand{\sphere}{\mathbb{S}^2}
      \newcommand{\modeint}{\mathcal{M}}
      \newcommand{\azimint}{\mathcal{N}}
      \newcommand{\ra}{\rightarrow}
      \newcommand{\mcal}[1]{\mathcal{#1}}
      \newcommand{\X}{\mathcal{X}}
      \newcommand{\Y}{\mathcal{Y}}
      \newcommand{\Z}{\mathcal{Z}}
      \newcommand{\x}{\mathbf{x}}
      \newcommand{\y}{\mathbf{y}}
      \newcommand{\z}{\mathbf{z}}
      \newcommand{\tr}{\mathrm{tr}}
      \newcommand{\sgn}{\mathrm{sgn}}
      \newcommand{\diag}{\mathrm{diag}}
      \newcommand{\Real}{\mathbb{R}}
      \newcommand{\sseq}{\subseteq}
      \newcommand{\ov}[1]{\overline{#1}}
      \DeclareMathOperator*{\argmax}{arg\,max}
      \DeclareMathOperator*{\argmin}{arg\,min}
      \)
    </span>

    <h1>7 &nbsp; The graphics pipeline, OpenGL, and WebGL</h1>
    <hr>

    <p>We have learned in the previous chapters about how to represent scenes and objects with triangle meshes. It is now time to learn about how we transform these representations into images that will be consumed by users. As mentioned previously in <a href="../chapter-01/index.html">Chapter 1</a>, the technical word for this process is "rendering." We have also mentioned the only rendering algorithm we will use in this book, the "graphics pipeline," and its particular implementation, "WebGL," several times. In this chapter, we will discuss what they are and how they work.</p>

    <h2>7.1 &nbsp; Rendering algorithms</h2>

    <p>First, let us specify precisely what a rendering algorithm is. A <b>rendering algorithm</b> takes as input (1) a description of a 3D scene and (2) a description of how the scene is viewed by an observer. It then outputs an image that depicts the scene through the aforementioned viewpoint. Metaphorically, a rendering algorithm is thus the inner working of a camera that takes snapshots of the scene.</p>

    <p>Let us take a closer look at the output. We learned in <a href="../chapter-02/">Chapter 2</a> that, most of the times, images that a rendering algorithm produces would be raster images. A raster image is made of a number of pixels. Hence, the main task of a rendering algorithm is to figure out the colors of all the pixels in the output color images. We also learned that the description of the scene defines an implicit continuous image from which we must construct the raster image from. A continous image contains much more information than a raster image can ever store, so a rendering algorithm is practically "summarizes" the continuous image into a raster one.</p>

    <p>Now, looking closer at the input, a scene would consist of a number of objects. We have decided that each of these objects would be represented by meshes that are in turn made out of primitives. We can think of these primitives as emitting light which has color, and this color should be recorded by a rendering algorithm in the output image if the observer can see the primitive from the input viewpoint. There are several reasons why a primitive, or some parts of it, may not be recorded by a rendering algorithm. First, the primitive may be out of the viewpoint's "field of view." For examples, a primitive that is behind the camera would not be seen by the camera. Second, the primitive might be behind another primitive from the camera's point of view. As a result, rendering involves figuring out, for each pixel, which primitive we should take the color from.</p>

    <p>In computer graphics, there are two main ways to organize computation to answer the question of "which primitive contributes to which pixel." This results in two types of rendering algorithms: <b>image-order rendering algorithms</b> and <b>object-order rendering algorithms.</b> Simplified pseudocode for the algorithms is given in Figure 7.1.</p>

    <figure class="figure">
    <table class="table table-bordered" width="100%">
        <tr class="table-primary">
            <td scope="col" style="text-align: center"><b>Image-order rendering algorithm</b></td>
        </tr>    
        <tr>
            <td>
                <ul style="list-style-type: none;">
                    <li>
                        <b>for each</b> pixel <b>in</b> output image <b>do</b>
                        <ul style="list-style-type: none;">
                            <li>
                                <b>for each</b> primitive <b>in</b> scene <b>do</b>
                                <ul style="list-style-type: none;">
                                    <li><b>if</b> primitive contributes to pixel <b>then</b>
                                        <ul style="list-style-type: none;">
                                            <li>Record primitive's contribution to pixel</li>
                                        </ul>
                                    </li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                </ul>
            </td>
        </tr>
        <tr class="table-primary">
            <td scope="col" style="text-align: center"><b>Object-order rendering algorithm</b></td>
        </tr>
        <tr>
            <td>
                <ul style="list-style-type: none;">
                    <li>
                        <b>for each</b> primitive <b>in</b> scene <b>do</b>                                
                        <ul style="list-style-type: none;">
                            <li>
                                <b>for each</b> pixel <b>in</b> output image <b>do</b>
                                <ul style="list-style-type: none;">
                                    <li><b>if</b> primitive contributes to pixel <b>then</b>
                                        <ul style="list-style-type: none;">
                                            <li>Record primitive's contribution to pixel</li>
                                        </ul>
                                    </li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                </ul>
            </td>
        </tr>
    </table>
    <figcaption class="figure-caption"><strong>Figure 7.1</strong> Simplified pseudocode of an image-order rendering algorithm and an object-order rendering algorithm.</figcaption>
    </figure>

    <p>The two algorithms are very similar: two for loops with the same loop body. The only difference between them is the order of the loops. Image-order algorithms' outer loops cycle through the pixels, while those for object-order algorithms cycle through the primitives. For the pseudocode in Figure 7.1, the two algorithms are equivalent. However, more advanced versions of the algorithms have more sophisticated ways of pruning useless pixel-primitive pairs to make computation faster.</p>

    <p>One of the most well-kwown image-order rendering algorithm is <b>ray casting</b>. The algorithm posits that the color of a pixel is the color of the light that travels along the ray that passes through the center of the pixel in the image plane. (Recall <a href="../chapter-02/index.html#figure-2p3">Figure 2.3</a>.) So, figure out the color, it finds the first primitive that intersects the ray and take the color from that primitive. A more sophisticated version of ray casting called <b>path tracing</b> takes into account that primitives can also reflects light that comes from light source or other primitives, and so it can spawn more rays from the intersection point until it finds a light source. Path tracing can generative high-quality rendering that looks photo-realistic, and it is the basis of rendering algorithms used in CG-animated movies and special effect shots in live-action films. Image-order rendering algorithms are usually implemented entirely in software, making it difficult for them to meet the strigent speed requirement of interactive applications.</p>

    <p>The "graphics pipeline" is the most well-known example of object-order rendering algorithms, and we will describe it in more details in the next section. In constrast to ray casting and path tracing, it has hardware that is designed specifically to run it: the <b>graphics processing units</b> (GPUs).<sup><a href="#fn:gpu">1</a></sup> Hardware acceleration makes it possible to render complex scenes in real-time, enabling applications such as computer games and interactive visualization.</p>

    <h2>7.2 &nbsp; The graphics pipeline</h2>

    <p>The graphics pipeline takes as input a specification of a scene. Here, unlike what we discussed previously, the specification of the camera and the camera's viewpoint is implicitly included in the scene specifiction, and we will discuss how this inclusion is accomplished momemtarily. The graphics pipeline assumes that objects in the scene are represented by meshes. Recall from <a href="../chapter-06/">Chapter 6</a>, that a mesh is a collection of vertices and a list of primitives that are constructed from these vertices. A scene can also have image data called "textures," but we will not discuss them until Chapter XXX. The graphics pipline must then produces a raster image of the scene. The memory that stores the output image is called the <b>framebuffer</b>. It is a special area of GPU memory that the color values written there would be displayed on the monitor connected to the GPU.</p>

    <p>The graphics pipeline convert meshes into a raster image with the following 5-step process.
    <ol>
      <li>Vertex processing.</li>
      <li>Primitive assembly.</li>
      <li>Rasterization.</li>
      <li>Fragment processing.</li>
      <li>Raster operation.</li>
    </ol>
    </p>

    <p>An overview of what each step does is shown in Figure 7.1. Let us now discuss each of the step in more details.</p>

    <figure class="figure">
      <figcaption class="figure-caption"><strong>Figure 7.1</strong> TODO: A figure that gives an overview of what each step of the graphics pipeline does.</figcaption>
    </figure> 

    <h3>7.2.1 &nbsp; Vertex Processing</h3>

    <p>The main goal of this step is to process vertex attributes so that they are easy for later steps to work on. The most important attribute to process is the position of the vertex. At the end of vertex processing, all positions must be transformed so that they becomes <b>normalized device coordinates</b> (NDCs). NDCs are coordinates in the <b>normalized device coordinate system</b>: a 3D coordinate system such that anything outside a certain volume is not visible in the output image. Different implementations of the graphics pipeline have different definitons of the "visible volume." For OpenGL and WebGL, it is the cube $\{ (x,y,z) : -1 \leq x,y,z \leq 1\}$. For DirectX, it is the box $\{ (x,y,z) : -1 \leq x, y \leq 1, 0 \leq  z \leq 1\}$.</p>

    <p>We said earlier that the graphics pipeline assumes that the viewpoint is implicitly included in the scene description. What we meant by this is that, in modern implementation of the graphics pipeline, it is the responsibility of the programmer to customize the vertex processing step so the transformation of vertex positions into NDCs would take into account the viewpoint and the camera. This customization is often done through writing a piece of code called the <b>vertex shader</b>, which can be thought of as a part of the scene's specification. We will discuss how to write vertex shaders to implement two frequently-used types of cameras in Chapter XXX. For now, just remember that simulating cameras can be done inside a vertex shader, and it is our responsibility to do so.</p>

    <h3>7.2.2 &nbsp; Primitive Assembly</h3>

    <p>The primitive assembly step process information about primitives. Mainly, it does the following.
    <ul>
      <li><b>Primitive simplification.</b> Complex primitives are converted into simpler "base primitives" (points, lines, and triangles). In WebGL, for examples, line strips and line loops are turned into individual lines. Triangle strips and triangle fans are turned into individual triangles.</li>

      <li><b>Face culling.</b> If all vertices of a primitive have NDCs that are outside the visible volume, the primitive is not visible in the output image, and so can be dropped from further processing because it would not have any impact on the output. Face culling refers to the removal of such primitives.</li>
    </ul>
    </p>

    <p>Modern implementations of the graphics pipeline also supports customization of the primitive assembly steps by allowing the programmer to write <b>tessllation shaders</b> and <b>geometry shaders</b>. However, we will not cover these aspects of the graphics pipeline in this book because they are not currently supported by WebGL.</p>
    
    <h3>7.2.3 &nbsp; Rasterization</h3>

    <p>Primivites that survive face culling (meaning that they are potentially visible in the output image if not occluded by other primitives) are sliced into rectangular areas, each has the size of a pixel. This process is called <b>rasterization</b>, and each generated rectangular area is called a <b>fragment</b>. A fragment can be thought of as a "potential pixel," whose visibility will be determined in a later step of the pipeline. Unlike the previous two steps, this step is often not customizable.</p>
 
    <p>An important function of the rasterization step is interpolation of vertex attributes. Recall from <a href="../chapter-05/index.html">Chapter 5</a> and <a href="../chapter-06/index.html">Chapter 6</a> that attributes such as color and vertex normals are interpolated when the graphics pipeline needs to fill in the space between the vertices. It is through rasterization that this interpolation is performed. We will discuss the specifics of how interpolation is performed later when we discuss OpenGL and WebGL.</p>

    <h3>7.2.4 &nbsp; Fragment Processing</h3>

    <p>This step processes all the fragments generated by the last step with the following two goals:
    <ol>
      <li>determining whether the fragment should be discarded (i.e., become totally invisible and dropped from further processing), and</li>
      <li>determining each fragment's RGBA color if it is not discard.</li>
    </ol>
    Modern implementations of the graphics pipeline allows full customization of the fragment processing step by allowing the programmer to write <b>fragment shaders.</b> Because the fragment shader determines colors of fragments, it is responsible for how objects appear to the viewer in the output image. In most interactive applications, the fragment shader would take into account information such as the light sources and the material properties of the surface.
    </p>

    <h3>7.2.5 &nbsp; Raster Operation</h3>

    <p></p>

    <h2>7.3 &nbsp; OpenGL and WebGL</h2>

    <hr>
    <div class="footnotes">
      <ol> 
            <li class="footnote" id="fn:gpu">
                <p>While GPUs are designed to run the graphics pipeline, modern GPUs can run many other types of algorithms. Currently, GPUs are the main workhorse of artificial intelligence because it can execute and train neural networks fast.</p>
            </li>
            <li class="footnote" id="fn:ndc-convension">
              <p>While GPUs are designed to run the graphics pipeline, modern GPUs can run many other types of algorithms. Currently, GPUs are the main workhorse of artificial intelligence because it can execute and train neural networks fast.</p>
          </li>
      </ol>
    </div>

    <hr>
    <p>
      <a href="../chapter-06/index.html"><<</a>
      <a href="../index.html">Contents</a>
    </p>
  </div>
  <script src="../js/bootstrap.bundle.min.js"></script>
  <script src="../js/littlefoot.js" type="application/javascript"></script>
  <script type="application/javascript">
    const { littlefoot } = require('littlefoot')

    const lf = littlefoot({
      activateOnHover: true,
      hoverDelay: 250,
    });
  </script>
</body>

</html>