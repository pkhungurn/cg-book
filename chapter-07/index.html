<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="../css/bootstrap.min.css" rel="stylesheet">
  <link href="../css/theme.css" rel="stylesheet">
  <!-- MathJax -->
  <script type="text/javascript">
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        tags: "ams",
      },
      svg: {
        fontCache: 'global'
      },
    };
  </script>
  <script type="text/javascript" id="MathJax-script" src="../mathjax/tex-mml-chtml.js"></script>
  <link rel="stylesheet"
      href="../highlight/styles/default.min.css">
  <script src="../highlight/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
    
  <title>Chapter 7</title>
</head>

<body>
  <div class="container" style="max-width: 640px;">
    <span style="visibility: hidden;">
      \(
      \def\sc#1{\dosc#1\csod}
      \def\dosc#1#2\csod{{\rm #1{\small #2}}}

      \newcommand{\dee}{\mathrm{d}}
      \newcommand{\Dee}{\mathrm{D}}
      \newcommand{\In}{\mathrm{in}}
      \newcommand{\Out}{\mathrm{out}}
      \newcommand{\pdf}{\mathrm{pdf}}
      \newcommand{\Cov}{\mathrm{Cov}}
      \newcommand{\Var}{\mathrm{Var}}

      \newcommand{\ve}[1]{\mathbf{#1}}
      \newcommand{\mrm}[1]{\mathrm{#1}}
      \newcommand{\ves}[1]{\boldsymbol{#1}}
      \newcommand{\etal}{{et~al.}}
      \newcommand{\sphere}{\mathbb{S}^2}
      \newcommand{\modeint}{\mathcal{M}}
      \newcommand{\azimint}{\mathcal{N}}
      \newcommand{\ra}{\rightarrow}
      \newcommand{\mcal}[1]{\mathcal{#1}}
      \newcommand{\X}{\mathcal{X}}
      \newcommand{\Y}{\mathcal{Y}}
      \newcommand{\Z}{\mathcal{Z}}
      \newcommand{\x}{\mathbf{x}}
      \newcommand{\y}{\mathbf{y}}
      \newcommand{\z}{\mathbf{z}}
      \newcommand{\tr}{\mathrm{tr}}
      \newcommand{\sgn}{\mathrm{sgn}}
      \newcommand{\diag}{\mathrm{diag}}
      \newcommand{\Real}{\mathbb{R}}
      \newcommand{\sseq}{\subseteq}
      \newcommand{\ov}[1]{\overline{#1}}
      \DeclareMathOperator*{\argmax}{arg\,max}
      \DeclareMathOperator*{\argmin}{arg\,min}
      \)
    </span>

    <h1>7 &nbsp; The graphics pipeline, OpenGL, and WebGL</h1>
    <hr>

    <p>We have learned in the previous chapters about how to represent scenes and objects with triangle meshes. It is now time to learn about how we transform these representations into images that will be consumed by users. As mentioned previously in <a href="../chapter-01/index.html">Chapter 1</a>, the technical word for this process is "rendering." We have also mentioned the only rendering algorithm we will use in this book, the "graphics pipeline," and its particular implementation, "WebGL," several times. In this chapter, we will discuss what they are and how they work.</p>

    <h2>7.1 &nbsp; Rendering algorithms</h2>

    <p>First, let us specify precisely what a rendering algorithm is. A <b>rendering algorithm</b> takes as input (1) a description of a 3D scene and (2) a description of how the scene is viewed by an observer. It then outputs an image that depicts the scene through the aforementioned viewpoint. Metaphorically, a rendering algorithm is thus the inner working of a camera that takes snapshots of the scene.</p>

    <p>Let us take a closer look at the output. We learned in <a href="../chapter-02/">Chapter 2</a> that, most of the times, images that a rendering algorithm produces would be raster images. A raster image is made of a number of pixels. Hence, the main task of a rendering algorithm is to figure out the colors of all the pixels in the output color images. We also learned that the description of the scene defines an implicit continuous image from which we must construct the raster image from. A continous image contains much more information than a raster image can ever store, so a rendering algorithm is practically "summarizes" the continuous image into a raster one.</p>

    <p>Now, looking closer at the input, a scene would consist of a number of objects. We have decided that each of these objects would be represented by meshes that are in turn made out of primitives. We can think of these primitives as emitting light which has color, and this color should be recorded by a rendering algorithm in the output image if the observer can see the primitive from the input viewpoint. There are several reasons why a primitive, or some parts of it, may not be recorded by a rendering algorithm. First, the primitive may be out of the viewpoint's "field of view." For examples, a primitive that is behind the camera would not be seen by the camera. Second, the primitive might be behind another primitive from the camera's point of view. As a result, rendering involves figuring out, for each pixel, which primitive we should take the color from.</p>

    <p>In computer graphics, there are two main ways to organize computation to answer the question of "which primitive contributes to which pixel." This results in two types of rendering algorithms: <b>image-order rendering algorithms</b> and <b>object-order rendering algorithms.</b> Simplified pseudocode for the algorithms is given in Figure 7.1.</p>

    <figure class="figure">
    <table class="table table-bordered" width="100%">
        <tr class="table-primary">
            <td scope="col" style="text-align: center"><b>Image-order rendering algorithm</b></td>
        </tr>    
        <tr>
            <td>
                <ul style="list-style-type: none;">
                    <li>
                        <b>for each</b> pixel <b>in</b> output image <b>do</b>
                        <ul style="list-style-type: none;">
                            <li>
                                <b>for each</b> primitive <b>in</b> scene <b>do</b>
                                <ul style="list-style-type: none;">
                                    <li><b>if</b> primitive contributes to pixel <b>then</b>
                                        <ul style="list-style-type: none;">
                                            <li>Record primitive's contribution to pixel</li>
                                        </ul>
                                    </li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                </ul>
            </td>
        </tr>
        <tr class="table-primary">
            <td scope="col" style="text-align: center"><b>Object-order rendering algorithm</b></td>
        </tr>
        <tr>
            <td>
                <ul style="list-style-type: none;">
                    <li>
                        <b>for each</b> primitive <b>in</b> scene <b>do</b>                                
                        <ul style="list-style-type: none;">
                            <li>
                                <b>for each</b> pixel <b>in</b> output image <b>do</b>
                                <ul style="list-style-type: none;">
                                    <li><b>if</b> primitive contributes to pixel <b>then</b>
                                        <ul style="list-style-type: none;">
                                            <li>Record primitive's contribution to pixel</li>
                                        </ul>
                                    </li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                </ul>
            </td>
        </tr>
    </table>
    <figcaption class="figure-caption"><strong>Figure 7.1</strong> Simplified pseudocode of an image-order rendering algorithm and an object-order rendering algorithm.</figcaption>
    </figure>

    <p>The two algorithms are very similar: two for loops with the same loop body. The only difference between them is the order of the loops. Image-order algorithms' outer loops cycle through the pixels, while those for object-order algorithms cycle through the primitives. For the pseudocode in Figure 7.1, the two algorithms are equivalent. However, more advanced versions of the algorithms have more sophisticated ways of pruning useless pixel-primitive pairs to make computation faster.</p>

    <p>One of the most well-known image-order rendering algorithm is <b>ray casting</b>. The algorithm posits that the color of a pixel is the color of the light that travels along the ray that passes through the center of the pixel in the image plane. (Recall <a href="../chapter-02/index.html#figure-2p3">Figure 2.3</a>.) So, figure out the color, it finds the first primitive that intersects the ray and take the color from that primitive. A more sophisticated version of ray casting called <b>path tracing</b> takes into account that primitives can also reflects light that comes from light source or other primitives, and so it can spawn more rays from the intersection point until it finds a light source. Path tracing can generative high-quality rendering that looks photo-realistic, and it is the basis of rendering algorithms used in CG-animated movies and special effect shots in live-action films. Image-order rendering algorithms are usually implemented entirely in software, making it difficult for them to meet the strigent speed requirement of interactive applications.</p>

    <p>The "graphics pipeline" is the most well-known example of object-order rendering algorithms, and we will describe it in more details in the next section. In constrast to ray casting and path tracing, it has hardware that is designed specifically to run it: the <b>graphics processing units</b> (GPUs).<sup><a href="#fn:gpu">1</a></sup> Hardware acceleration makes it possible to render complex scenes in real-time, enabling applications such as computer games and interactive visualization.</p>

    <p>Implementations of the graphics pipeline usually come in the form of <b>Application Programming Interfaces</b> (APIs), which are libraries of functions and data structures that programmers can use to build their own applications. Well-known implementations include  <a href="https://en.wikipedia.org/wiki/DirectX">DirectX</a>, <a href="https://en.wikipedia.org/wiki/Vulkan">Vulcan</a>, and <a href="https://en.wikipedia.org/wiki/Metal_(API)">Metal</a>. In this book, we will focus on <a href="https://en.wikipedia.org/wiki/OpenGL">OpenGL</a> and its web-based offshoot, <a href="https://en.wikipedia.org/wiki/WebGL">WebGL</a>. These implementations all rely on GPUs to perform almost of all its operations. As a result, they can be regarded as interfaces to the GPUs that allow 3D rendering.</p>

    <h2>7.2 &nbsp; The graphics pipeline</h2>

    <p>The graphics pipeline takes as input a specification of a scene. Here, unlike what we discussed previously, the specification of the camera and the camera's viewpoint is implicitly included in the scene specifiction, and we will discuss how this inclusion is accomplished momentarily. The graphics pipeline assumes that objects in the scene are represented by meshes. Recall from <a href="../chapter-06/">Chapter 6</a>, that a mesh is a collection of vertices and a list of primitives that are constructed from these vertices. A scene can also have image data called "textures," but we will not discuss them until Chapter XXX. The graphics pipline must then produces a raster image of the scene. The memory that stores the output image is called the <b>framebuffer</b>. It is a special area of GPU memory that the color values written there would be displayed on the monitor connected to the GPU.</p>

    <p>The graphics pipeline convert meshes into a raster image with the following 5-step process.
    <ol>
      <li>Vertex processing.</li>
      <li>Primitive assembly.</li>
      <li>Rasterization.</li>
      <li>Fragment processing.</li>
      <li>Raster operation.</li>
    </ol>
    </p>

    <p>An overview of what each step does is shown in Figure 7.2. Let us now discuss each of the step in more details.</p>

    <figure class="figure">
      <figcaption class="figure-caption"><strong>Figure 7.2</strong> TODO: A figure that gives an overview of what each step of the graphics pipeline does.</figcaption>
    </figure> 

    <h3>7.2.1 &nbsp; Vertex Processing</h3>

    <p>The main goal of this step is to process vertex attributes so that they are easy for later steps to work on. The most important attribute to process is the position of the vertex. At the end of vertex processing, all positions must be transformed so that they becomes <b>normalized device coordinates</b> (NDCs). NDCs are coordinates in the <b>normalized device coordinate system</b>: a 3D coordinate system such that anything outside a certain volume is not visible in the output image. Different implementations of the graphics pipeline have different definitons of the "visible volume." For OpenGL and WebGL, it is the cube $\{ (x,y,z) : -1 \leq x,y,z \leq 1\}$. For DirectX, it is the box $\{ (x,y,z) : -1 \leq x, y \leq 1, 0 \leq  z \leq 1\}$.</p>

    <p>We said earlier that the graphics pipeline assumes that the viewpoint is implicitly included in the scene description. What we meant by this is that, in modern implementation of the graphics pipeline, it is the responsibility of the programmer to customize the vertex processing step so the transformation of vertex positions into NDCs would take into account the viewpoint and the camera. This customization is often done through writing a piece of code called the <b>vertex shader</b>, which can be thought of as a part of the scene's specification. We will discuss how to write vertex shaders to implement two frequently-used types of cameras in Chapter XXX. For now, just remember that simulating cameras can be done inside a vertex shader, and it is our responsibility to do so.</p>

    <h3>7.2.2 &nbsp; Primitive Assembly</h3>

    <p>The primitive assembly step process information about primitives. Mainly, it does the following.
    <ul>
      <li><b>Primitive simplification.</b> Complex primitives are converted into simpler "base primitives" (points, lines, and triangles). In WebGL, for examples, line strips and line loops are turned into individual lines. Triangle strips and triangle fans are turned into individual triangles.</li>

      <li><b>Face culling.</b> If all vertices of a primitive have NDCs that are outside the visible volume, the primitive is not visible in the output image, and so can be dropped from further processing because it would not have any impact on the output. Face culling refers to the removal of such primitives.</li>
    </ul>
    </p>

    <p>Modern implementations of the graphics pipeline also supports customization of the primitive assembly steps by allowing the programmer to write <b>tessllation shaders</b> and <b>geometry shaders</b>. However, we will not cover these aspects of the graphics pipeline in this book because they are not currently supported by WebGL.</p>
    
    <h3>7.2.3 &nbsp; Rasterization</h3>

    <p>Primivites that survive face culling (meaning that they are potentially visible in the output image if not occluded by other primitives) are sliced into rectangular areas, each has the size of a pixel. This process is called <b>rasterization</b>, and each generated rectangular area is called a <b>fragment</b>. A fragment can be thought of as a "potential pixel," whose visibility will be determined in a later step of the pipeline. Unlike the previous two steps, this step is often not customizable.</p>
 
    <p>An important function of the rasterization step is interpolation of vertex attributes. Recall from <a href="../chapter-05/index.html">Chapter 5</a> and <a href="../chapter-06/index.html">Chapter 6</a> that attributes such as color and vertex normals are interpolated when the graphics pipeline needs to fill in the space between the vertices. It is through rasterization that this interpolation is performed. We will discuss the specifics of how interpolation is performed later when we discuss OpenGL and WebGL.</p>

    <h3>7.2.4 &nbsp; Fragment Processing</h3>

    <p>This step processes all the fragments generated by the last step with the following two goals:
    <ol>
      <li>determining whether the fragment should be discarded (i.e., become totally invisible and dropped from further processing), and</li>
      <li>determining each fragment's RGBA color if it is not discard.</li>
    </ol>
    Modern implementations of the graphics pipeline allows full customization of the fragment processing step by allowing the programmer to write <b>fragment shaders.</b> Because the fragment shader determines colors of fragments, it is responsible for how objects appear to the viewer in the output image. In most interactive applications, the fragment shader would take into account information such as the light sources and the material properties of the surface.
    </p>

    <a name="raster-operation"></a>
    <h3>7.2.5 &nbsp; Raster Operation</h3>

    <p>Recall from Section 7.5.2 that the "scissor test" is </p>

    <p>The fragment processing step can produce multiple fragments that occupy the same pixel location. The raster operation step process these fragments into a single color value. The step can be divided into two important substeps.</p>

    <ul>
      <li><b>Culling tests.</b> The fragments are tested to see whether they should be discarded or not. There are multiple types of tests that can performed on this steps. These include the "scissor test," "stencil test," and the "depth test." By far the most important test is the <b>depth test</b> where the pipeline determines whether the fragment should be discarded based on whether the fragment is "behind" any other fragments. We will cover the depth test in more details in Chapter XXX.</li>

      <li><b>Blending.</b> If a fragment passes all the culling tests, its color is written to the framebuffer. The default behavior is to overwrite whatever the color information is already there at the pixel localtion. However, the graphics pipeline also allows other behaviors that combine the fragment's color with the color that has already been recorded. These combining behaviors are referred to collectively as "blending." We will also cover blending in more details in Chapter XXX.</li>
    </ul>

    <p>Different implementations of the graphics pipeline may differ on how they implement the raster operation step. Some may have more culling tests, blending operations, or add entirely new operations altogether. </p>

    <h2>7.3 &nbsp; OpenGL and WebGL</h2>

    <p><b>OpenGL</b> stands for "<b>Open</b> <b>G</b>raphics <b>L</b>ibrary." It is originally developed by <a href="https://en.wikipedia.org/wiki/Silicon_Graphics">Silicon Graphics</a>, a well-known tech company that unfortunately went bankrupt in 2009. A organization called the <a href="https://en.wikipedia.org/wiki/Khronos_Group">Khronos Group</a> has been responsible for development and maintenance of OpenGL since 2006.</p>

    <p>"OpenGL" refers to the library that is written in the C programming language and intended to be used to develop native applications for personal computers. A spinoff library called <a href="https://en.wikipedia.org/wiki/OpenGL_ES">OpenGL for Embedded Systems</a> (abbreviated as "OpenGL ES") was released in 2003. It targets less powerful devices such as video game consoles, tablets, and smartphones.</p>

    <p><b>WebGL</b> is a spinoff of OpenGL ES for web applications. It is packaged as a Javascript library and is generally available in modern web browsers such as <a href="https://www.google.com/chrome/">Chrome</a>, <a href="https://www.mozilla.org/en-US/firefox/new/">Firefox</a>,  <a href="https://www.apple.com/safari/">Safari</a>, and <a href="https://www.microsoft.com/en-us/edge">Edge</a> without the need to install any extra software. In this book, we will use <a href="https://registry.khronos.org/webgl/specs/latest/2.0/"> WebGL version 2.0</a>, which has now been supported by all the aforementioned browsers above since 2022.</p>

    <p>Now, let us discuss details of the graphics pipeline's implementation that is specific to WebGL.</p>

    <a name="programs-and-shaders"></a>
    <h3>7.3.1 &nbsp; Programs and Shaders</h3>

    <p>There are two parts to writing problems WebGL. The first part is using Javascript to call functions in the WebGL to do its job. This includes preparing an area in the web page that WebGL will draw the primitives to, sending the data to the GPU, altering various settings of the graphics pipeline, and then asking WebGL to draws the primitives we want to see. We will discuss in the next chapters how to use these functions to create the simplest computer graphics programs in the next Chapter.</p>

    <p>The second part is customizing the graphics pipeline by writing <b>GLSL programs</b>. Here, a GLSL program is an object that contains as its constituent parts two shaders that WebGL 2.0 allows you to write: the <b>vertex shader</b> and the <b>fragment shader</b>. Shaders are written in a programming language called the <b>OpenGL Shading Language</b>, abbreviated as "GLSL." Like WebGL, there are multiple versions of the GLSL language. In this book, we will use <a href="https://registry.khronos.org/OpenGL/specs/es/3.0/GLSL_ES_Specification_3.00.pdf">GLSL ES version 3.0</a>, which is the latest version supported by WebGL 2.0. The syntax of the GLSL language is similar to languages in the C family, and we will discuss it in more details in Chapter XXX.</p>

    <p>Now, we mentioned earlier that modern implementations of the graphics pipeline allows the user to customize the pipeline to their needs. This kind of implies that there can be default behaviors to which the implication would fall back to if the user does not do any configurations. However, for WebGL 2.0, customizing the graphics pipeline with a GLSL program is a requirement if one wants to do anything more complicated than clearning the screen. As a result, if we want to draw any primitive at all, we must write two shaders in GLSL, combine them into a GLSL program, and have WebGL execute it for us.</p>

    <h3>7.3.2 &nbsp; Vertex Shaders</h3>

    <p>Recall that a vertex shader is a piece of code that customizes the vertex processing step of the graphics pipeline. Moreover, the main output of the vertex processing step is the NDCs corresponding to the vertex positions. WebGL has its own convention on how the NDCs are computed, and we must understand it in order for us to write effective shaders.</p>

    <p>The main data that WebGL expects a vertex shader to output is the positions of the vertices in <b>clip space</b>, and these positions are referred to as <b>clip space coordinates</b>. Clip space coordinates are <i>homogeneous coordinates</i> (<a href="../chapter-04/index.html#homogeneous-coordinates">Section 4.3 of Chapter 4</a>), which means that they have 4 components: $x$, $y$, $z$, and $w$. WebGL will convert clip space coordinates to NDCs by itself, and this part the user cannot change.</p>

    <p>The process that WebGL computes NDCs from clip space coordinates is called the <b>perspective divide</b>. Suppose that the clip space coordinates that the vertex shader outputs is $P_{\mrm{clip}} = (x_{\mrm{clip}}, y_{\mrm{clip}}, z_{\mrm{clip}}, w_{\mrm{clip}})$. WebGL would compute the NDC, denoted by $P_{\mrm{ndc}} = (x_{\mrm{ndc}}, y_{\mrm{ndc}}, z_{\mrm{ndc}})$ by dividing all clip space coordinates with the $z$-coordinate:
    \begin{align*}
      \begin{bmatrix}
        x_{\mrm{ndc}} \\ 
        y_{\mrm{ndc}} \\
        z_{\mrm{ndc}}        
      \end{bmatrix}
      = \begin{bmatrix}
      x_{\mrm{clip}} / w_{\mrm{clip}} \\ 
      y_{\mrm{clip}} / w_{\mrm{clip}} \\
      z_{\mrm{clip}} / w_{\mrm{clip}}       
      \end{bmatrix}.
    \end{align*}
    After the NDCs are computed, they would then be used in primitive assembly.</p>

    <p>A question that might come to the reader's mind right now is how we are supposed to write vertex shaders so they they output the right clip space coordinates. The answer is that there is a standard way to do it, and that computer graphics applications rarely deviate from this method. It involves multiplying the coordinates with matrices and the technical term for this is "transforming" them. There are three main types of transformations. One is used to put objects into a scene (<b>modeling transformation</b>), one is used to set up the camera's viewpoint (<b>view transformation</b>), and the last is used to project 3D objects to a 2D plane (<b>projective transformation</b> or <b>projection</b>). We will learn about the first two types of transformations in Chapter XXX and Chapter XXX, and we will learn about projections in Chapter XXX.</p>

    <p>However, for the next few chapters, we will not have the tools to do the "standard transformation pipeline" above yet. We will, for now, write vertex shaders so that they directly output clip space coordinates. Moreover, we will always set the $w$-coordinate to $1$ so that the NDCs are equal to the $xyz$-components of the clip space coordinates. Recall that WebGL considers any vertices whose NDCs are out of the cube $\{(x,y,z) : -1 \leq x,y,z \leq 1\}$</p> to be invisible to the camera. As a result, we must make sure that all primitives we want to show have positions in the $[-1,1]$ range.</p>
    
    <p>Other than the clip space coordinates of the vertex positons, the vertex shader can output other vertex attributes, and these are typically refer to as <b>varying variables</b>, and we will learn how to use them in Chapter XXX. These varying variables are interpolated by the rasterization step and then passed to the fragment shader.</p>

    <h3>7.3.3 &nbsp; Fragment Shaders</h3>

    <p>Recall that the rasterization step chops primivites into square areas, each having a size of a pixel, called framents. With each fragment, it attaches interpolated values of varying variables. Then, the graphics pipeline would transition to the fragment processing step, invoking the fragment shader on each fragment in order to process them.</p>

    <p>The fragment shader receives as input interpolated varying variables. It also have access to image data called "textures" (Chapter XXX) that can be set up by Javascript code. It must then process these information and perform one of the two actions previously discussed in Section 7.2.4.</p>

    <p>Because fragment shaders decide the fragment colors, it plays a central role in determining how objects appear in images. Hence, it must take into account information that are related to appearance, and these include the object's colors, textures, and how the object's surface reflect or emit light. It must also take into account the intensity and the direction in which light comes to illuminate the object. We will learn about how to model light interaction in Chapter XXX and Chapter XXX.</p>

    <h2>7.4 &nbsp; Summary</h2>

    <ul>
      <li>A <b>rendering algorithm</b> takes a description of a scene and a description of how the scene is viewed. It then creates a image of the scene accordingly as if taking a photograph.</li>

      <li>In computer graphics, scenes are typically described as a collection of primitives. Images are typically raster images, which is a collection of pixels.</li>

      <li>There are two main types of rendering algorithms based on the order of how primitives are pixels are iterated over.
      <ol>
        <li>An <b>image order rendering algorithm</b>'s main loop iterates over the pixels. For each pixel, it finds all primitives that contribute to the pixel's color and accumulate the contribution. <b>Ray tracing</b> and <b>path tracing</b> are examples of image order rendering algorithms.</li>

        <li>An <b>object order rendering algorithm</b>'s main loop iterates over the primitives. For each primitive, it finds all pixels that the primitive overlap with and write the contribution to the pixel.</li>
      </ol>
      </li>

      <li>The <b>graphics pipeline</b> is an object order rendering algorithm that is widely used in real-time applications such as 3D modelers and computer games. It is often executed by the graphics processing units (GPUs) so as to satisfy real-time constraints.</li>

      <li>The graphics pipeline renders a scene using a 5-ste process, but the main rendering step is <b>rasterization</b>: identifying the pixels that a primitive overlaps with and slicing the primitive small areas call <b>fragments</b> that correspond to those pixels.</li>

      <li>Implementations of the graphics pipeline usually come in the form of <b>application programming interfaces</b> (APIs) that are to be used by programmers to write computer graphics programs. Well-known APIs include DirectX, Vulcan, Metal, and OpenGL.</li>

      <li><b>OpenGL</b> is an implementation of the graphics pipeline originally created by Silocon Graphics and now maintained by the Khronos Group.</li>

      <li><b>WebGL</b>, the graphics API that we shall study in this book, is a version of OpenGL that runs in the web browser. Functions and data structures in the API are accessible through the Javascript language that most browsers can interpret.</li>

      <li>In order to create a graphics application with WebGL, one almost always have to customize the graphics pipeline by writing an <b>OpenGL program</b> in the <b>OpenGL Shading Language</b> (GLSL).</li>

      <li>An OpenGL program is a collection of two pieces of code.
      <ul>
        <li>The <b>vertex shader</b> is used to customize how the graphics pipeline would process vertex data. The main output of the vertex shader is the vertex's position in <b>clip space</b>.</li>
        <li>The <b>fragment shader</b> is used to specify the color of each fragment created by the rasterization step. The only output of the fragment shader is the fragment's color.</li>
      </ul>
      </li>
    </ul>

    <hr>
    <div class="footnotes">
      <ol> 
            <li class="footnote" id="fn:gpu">
                <p>While GPUs are designed to run the graphics pipeline, modern GPUs can run many other types of algorithms. Currently, GPUs are the main workhorse of artificial intelligence because it can execute and train neural networks fast.</p>
            </li>
      </ol>
    </div>

    <hr>
    <p>
      <a href="../chapter-06/index.html"><<</a>
      <a href="../index.html">Contents</a>
    </p>
  </div>
  <script src="../js/bootstrap.bundle.min.js"></script>
  <script src="../js/littlefoot.js" type="application/javascript"></script>
  <script type="application/javascript">
    const { littlefoot } = require('littlefoot')

    const lf = littlefoot({
      activateOnHover: true,
      hoverDelay: 250,
    });
  </script>
</body>

</html>